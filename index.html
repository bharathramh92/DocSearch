<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project Report</title>
</head>
<body>
<h1>DocSearch, </h1><h5>by Bharathram Hariharan, 800886307 and Midhun Mathew Sunny, 800886599</h5>
<h1>Introduction</h1>
<p>This project is a demonstration of creating an internal search engine solely based on cloud computing
    tools and techniques. We have used Spark for big data processing. This project is intended for generic
    data and data definition is defined by constants and hence this project could be reused for any type
    of data. <a href="http://startupbros.com/rank-amazon/">This Article</a> gives us an insight about how
    the Amazon search algorithm could be.</p>
<h4>Objective</h4>
<p>Perform data/item retrieval for e-commerce data set from their meta data. For this project, the data set
is of books. For more information on how the meta data for the books should be chosen,
    <a href="http://www.thebookdesigner.com/2012/05/self-publishing-basics-introduction-to-metadata/">refer
        this link</a></p>
<h4 id="dataset">Data-set</h4>
Each record has meta data for a book in json format and following are the entities.
<ul>
    <li><b>Title</b> Title of the book</li>
    <li><b>Authors</b> List of Authors for the book  </li>
    <li><b>Publishers</b> Book Publisher  </li>
    <li><b>Description</b> Book Description </li>
    <li><b>Categories</b> List of Categories/Sub-Categories </li>
    <li><b>ISBN_10</b>  </li>
    <li><b>ISBN_13</b>   </li>
    <li><b>KeyWords</b> Special terms people search for finding this book </li>
</ul>
<p>Sample format for a book record</p>
<p><code>{"uKQ0CgAAQBAJ": {"imageLinks": {"smallThumbnail": "http://books.google.com/books/content?id=uKQ0CgAAQBAJ&printsec=frontcover&img=1&zoom=5&edge=curl&source=gbs_api", "thumbnail": "http://books.google.com/books/content?id=uKQ0CgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api"}, "categories": ["Fiction"], "description": "Meredith has never considered herself submissive even though her greatest fantasy is being pleasured against her will. When Mark orders her to her knees the first time, she can\u2019t get there fast enough\u2014and then hates herself afterward for losing control. A", "publisher": "Ellora's Cave Publishing Inc", "ISBN_13": "9781419994289", "keyWords": ["Meredith", "Mark"], "infoLink": "http://books.google.com/books?id=uKQ0CgAAQBAJ&dq=go&as_pt=BOOKS&hl=&source=gbs_api", "authors": ["L.E. Chamberlin"], "ISBN_10": "141999428X", "maturityRating": "NOT_MATURE", "title": "The Rewards of Letting Go"}}</code></p>

<p id="views_rdd">Additionally, view count for each record is provided separately.</p>
<p>Sample view data format: <code>('NLngYyWFl_YC', 1292215)</code></p>
<p></p>
<h1>Project flow</h1>
<ul>
    <li><a href="#data_extraction">Data extraction</a></li>
    <li><a href="#key_gen">Keywords generation</a></li>
    <li><a href="#index_gen">Indexing</a></li>
    <li><a href="#query_gen">Querying</a></li>
</ul>
<h2 id="data_extraction">Data Extraction</h2>
<p>
    Google books api was used to get the book details. For making query in google books api, search terms are to
    be provided. Search terms we took was the category names. This category name was crawled from goodreads.com
</p>
<p>
    InitialDataExtraction/category_list_backup.py has a list of category name which we got from goodreads.com.
    A total of <b>802</b> category names were retrieved.
</p>
<p>
    Each query from google books api can yield up to <b>40</b> records. Typically, for a search term, total number of
    available records would in the range of 1000 - 3000.
</p>
<h4>Challenges in data extraction phase</h4>
<ul>
    <li>
        <p>Duplicate entry detection</p>
        <p>This was resolved by keeping a record of already downloaded data. This was important because the
        program had to be executed over a span of days with breaks due to network blocking problem by Google.</p>
    </li>
    <li>
        <p>Blocking of our IP by Google</p>
        <p>
            Easiest solution was to wait for a certain period of time before re-requesting and automating this task.
        </p>
    </li>
</ul>

<h2 id="key_gen">Keywords generation</h2>

<p>KeyWord sub project was to retrieve keywords from description of the books.</p>
<p>Tried two approaches</p>
<ul>
    <li>Stanford Named Entity Recognition(NER)</li>
    <li>Proper Noun extraction</li>
</ul>
<h4>Insights from two methods</h4>
<p>Keywords from NER's found to be unreliable and Proper Nouns gave better results. Issues with NER's were,
    many unwanted words were generated and  Improper entity definition for the terms and hence unreliable. However,
    Proper Nouns were not giving False Positives as in the former case
</p>

<p>Json data manipulation was performed using gson library</p>
<p>Input to this method was the data retrieved from google api.</p>
<p>Output is an added entity called keyWords in book record.</p>

<br/>
<h2 id="index_gen">Indexing</h2>
<p>This method generates an inverted index for each book record and are stored in index_rdd map.</p>
<p>Input for this method is referenced by id_doc_rdd_raw and its data format is as mentioned <a href="#dataset">above</a>.</p>
<p>index_rdd map has indexed term mapped to corresponding document id along with the zone/entity name</p>
<p>Following is an example.</p>
<p id="index_rdd"><code>('aceline', (('K3NLoAEACAAJ', 'keyWords'), ('7DiNBwAAQBAJ', 'title')))</code></p>
<p>'aceline' is the indexed term, K3NLoAEACAAJ is the doc_id, keyWords was the zone and likewise.</p>
<h4>Brief walk through about Map Reduce in this phase</h4>
<ul> <h4>Map Phase</h4>
    <li>JSON data is converted to python data object </li>
    <li>Keys are entities/zones, and value is their corresponding value in string of list format</li>
    <li>All string values are converted to list</li>
    <li>Lemmatized term for certain entities are taken for some of the entities as defined
        in the IndexConstants.LEMMA_ENTITIES constant. </li>
    <li>Using flatMap, term is mapped to a tuple of document id and entity/zone name</li>
    <h4>Reduce Phase</h4>
    <li>This is a reduce by key phase</li>
    <li>Addition of tuples of values(tuple of document id and entity/zone name) from map phase appended together
        for a key(term)</li>
</ul>
<br/>
<h2 id="query_gen">Querying</h2>
<p>Each entity needs to have different weights. If 2 documents have the search term present in title and keyWords
    respectively, we have to give more priority to the document whose title matched.
    Likewise, more visited document has to be given higher priority.</p>
<p>Input for this method is the <a href="#index_rdd">output</a> from Index method
    and <a href="#views_rdd">views_rdd</a>.</p>
<p>Following is a weight map which was used in our model.</p>
<p><code>
    MODEL_WEIGHTS = {ISBN_10: .3, ISBN_13: .3, AUTHORS: .18, PUBLISHER: .15, TITLE: .15, CATEGORIES: .12,
    KEYWORDS: .1, VIEWS: .5}
</code></p>
<p>Above weights are formulated after lot of query search experiments and intuitions.</p>
<p>View counts are generated for only select few documents.</p>
<p>Output of this phase is a list of tuple of document_id and score in sorted order.</p>
<br/>
<p>Following table shows the accuracy improvement in the search while adding view count as a parameter.</p>
<p>Search term for this example was "thomas cormen"</p>
<table border="1">
<tr><td>Document_id </td><td> With View Rank </td><td> With View Rank</td></tr>
<tr><td>i-bUBQAAQBAJ </td><td> 1 </td><td> 8</td></tr>
<tr><td>h2xRPgAACAAJ </td><td> 2 </td><td> 18</td></tr>
<tr><td>g8d3PwAACAAJ </td><td> 3 </td><td> 12</td></tr>
<tr><td>z73loAEACAAJ </td><td> 4 </td><td> 14</td></tr>
<tr><td>aefUBQAAQBAJ </td><td> 5 </td><td> 2</td></tr>
<tr><td>jUF9BAAAQBAJ </td><td> 6 </td><td> 5</td></tr>
<tr><td>pUGYSQAACAAJ </td><td> 7 </td><td> 23</td></tr>
<tr><td>r-oGPLclJc0C </td><td> 8 </td><td> 6</td></tr>
<tr><td>5kmsSgAACAAJ </td><td> 9 </td><td> 24</td></tr>
<tr><td>46JvnQEACAAJ </td><td> 10 </td><td> 3</td></tr>
<tr><td>unDKrQEACAAJ </td><td> 11 </td><td> 11</td></tr>
<tr><td>U2P3NwAACAAJ </td><td> 12 </td><td> 7</td></tr>
<tr><td>YsE2pwAACAAJ </td><td> 13 </td><td> 16</td></tr>
<tr><td>JtlDAAAACAAJ </td><td> 14 </td><td> 19</td></tr>
<tr><td>Ew--ngEACAAJ </td><td> 15 </td><td> 29</td></tr>
<tr><td>ZLN9CQAAQBAJ </td><td> 16 </td><td> 32</td></tr>
<tr><td>yfnloQEACAAJ </td><td> 17 </td><td> 25</td></tr>
<tr><td>EUrXoAEACAAJ </td><td> 18 </td><td> 9</td></tr>
<tr><td>COxdlgEACAAJ </td><td> 19 </td><td> 10</td></tr>
<tr><td>7cNgPwAACAAJ </td><td> 20 </td><td> 20</td></tr>
<tr><td>F3anBQAAQBAJ </td><td> 21 </td><td> 1</td></tr>
<tr><td>NLngYyWFl_YC </td><td> 22 </td><td> 15</td></tr>
<tr><td>m2QwnwEACAAJ </td><td> 23 </td><td> 13</td></tr>
<tr><td>dEplHQAACAAJ </td><td> 24 </td><td> 21</td></tr>
<tr><td>Jwr8jwEACAAJ </td><td> 25 </td><td> 17</td></tr>
<tr><td>NdSXoAEACAAJ </td><td> 26 </td><td> 4</td></tr>
<tr><td>AGAiAgAAQBAJ </td><td> 27 </td><td> 22</td></tr>
<tr><td>he2koAEACAAJ </td><td> 28 </td><td> 27</td></tr>
<tr><td>3QU2SgAACAAJ </td><td> 29 </td><td> 30</td></tr>
<tr><td>nunPngEACAAJ </td><td> 30 </td><td> 31</td></tr>
<tr><td>0hX5ygAACAAJ </td><td> 31 </td><td> 26</td></tr>
<tr><td>mmFGSwAACAAJ </td><td> 32 </td><td> 28</td></tr>
</table>

<h4>Brief walk through about filter phase after Querying</h4>
<ul>
    <li>Each query term is separated by whitespace</li>
    <li>Query term with hiphen are taken as a special case. If the splits(at hiphen, eg: 930-123) for this case are numbers, hiphen
    is removed and combined together(930-123 becomes 930123). If the splits are alphanumeric(abc-xyz), each split is taken separately(abc-xyz becomes 'abc' and 'xyz')</li>
    <li>Lemmatization for each term is done as well in search list</li>
    <li>AND operation is performed between all query terms and the results obtained after AND operation is searched through inverted indices. </li>
    <li>View count for the documents obtained from inverted index data is also considered.</li>
    <li>A final ranking is done for these documents based on the weights for each query term and view count.</li>
    <li>Sorting of above result in descending order gives the final serach outputs.</li>

</ul>

<h2 id="results">Results</h2>
<p>Mini search engine was implemented successfully. </p>
<p>Note that we had a total of 120,000 books in our sample data-set and hence a direct comparison between an established
search engine like google wont give exact accuracy rate. However, for the books which have, following are findings.</p>
<p>Following table shows the comparison of our search engines rank with the books which came 1st in google</p>
<table border="1">
    <tr><td>Term </td><td> Document Id </td><td> Rank in this system</td></tr>
    <tr><td>clrs </td><td> i-bUBQAAQBAJ </td><td> 1</td></tr>
    <tr><td>deception point </td><td> BjFZ0UQOUCgC </td><td> 1</td></tr>
    <tr><td>bleak house </td><td> OeD4XMKlxUIC </td><td> 1</td></tr>
</table>
</body>
</html>
