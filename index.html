<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project Report</title>
</head>
<body>
<h1>Introduction</h1>
<p>This project is a demonstration of creating an internal search engine solely based on cloud computing
    tools and techniques. We have used Spark for big data processing.</p>
<h4>Objective</h4>
<p>Perform data/item retrieval for e-commerce data set from their meta data. For this project, the data set
is of books.</p>
<h4 id="dataset">Data-set</h4>
Each record has meta data for a book in json format and following are the entities.
<ul>
    <li>Title</li>
    <li>Authors</li>
    <li>Publishers</li>
    <li>Description</li>
    <li>Categories</li>
    <li>ISBN_10</li>
    <li>ISBN_13</li>
</ul>
<p>Sample format for a book record</p>
<p><code>{"uKQ0CgAAQBAJ": {"imageLinks": {"smallThumbnail": "http://books.google.com/books/content?id=uKQ0CgAAQBAJ&printsec=frontcover&img=1&zoom=5&edge=curl&so
urce=gbs_api", "thumbnail": "http://books.google.com/books/content?id=uKQ0CgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api"}, "catego
ries": ["Fiction"], "description": "Meredith has never considered herself submissive even though her greatest fantasy is being pleasured against her wi
ll. When Mark orders her to her knees the first time, she can\u2019t get there fast enough\u2014and then hates herself afterward for losing control. A"
, "publisher": "Ellora's Cave Publishing Inc", "ISBN_13": "9781419994289", "keyWords": ["Meredith", "Mark"], "infoLink": "http://books.google.com/books
?id=uKQ0CgAAQBAJ&dq=go&as_pt=BOOKS&hl=&source=gbs_api", "authors": ["L.E. Chamberlin"], "ISBN_10": "141999428X", "maturityRating": "NOT_MATURE", "title
": "The Rewards of Letting Go"}}</code></p>

<p id="views_rdd">Additionally, view count for each record is provided separately.</p>
<p>Sample view data format: <code>('NLngYyWFl_YC', 1292215)</code></p>
<p></p>
<h1>Project flow</h1>
<ul>
    <li>Data extraction</li>
    <li>Keywords generation</li>
    <li>Indexing</li>
    <li>Querying</li>
</ul>
<h3>Data Extraction</h3>
<p>
    Google books api was used to get the book details. For making query in google books api, search terms are to
    be provided. Search terms we took was the category names. This category name was crawled from goodreads.com
</p>
<p>
    InitialDataExtraction/category_list_backup.py has a list of category name which we got from goodreads.com.
    A total of <b>802</b> category names were retrieved.
</p>
<p>
    Each query from google books api can yield up to <b>40</b> records. Typically, for a search term, total number of
    available records would in the range of 1000 - 3000.
</p>
<h4>Challenges in data extraction phase</h4>
<ul>
    <li>
        <p>Duplicate entry detection</p>
        <p>This was resolved by keeping a record of already downloaded data. This was important because the
        program had to be executed over a span of days with breaks due to network blocking problem by Google.</p>
    </li>
    <li>
        <p>Blocking of our IP by Google</p>
        <p>
            Easiest solution was to wait for a certain period of time before re-requesting and automating this task.
        </p>
    </li>
</ul>

<h3>Keywords generation</h3>

<p>KeyWord sub project was to retrieve keywords from description of the books.</p>
<p>Tried two approaches</p>
<ul>
    <li>Stanford Named Entity Recognition(NER)</li>
    <li>Proper Noun extraction</li>
</ul>
<h4>Insights from two methods</h4>
<p>Keywords from NER's found to be unreliable and Proper Nouns gave better results. Issues with NER's were,
    many unwanted words were generated and  Improper entity definition for the terms and hence unreliable. However,
    Proper Nouns were not giving False Positives as in the former case
</p>

<p>Json data manipulation was performed using gson library</p>
<p>Input to this method was the data retrieved from google api./p>
<p>Output is an added entity called keyWords in book record.</p>

<br/>
<h3>Indexing</h3>
<p>This method generates an inverted index for each book record and are stored in index_rdd.</p>
<p>Input for this method is id_doc_rdd_raw and the data format is mentioned <a href="#dataset">above</a>.</p>
<p>index_rdd map has indexed term mapped to corresponding document id along with the zone/entity name</p>
<p>Following is an example.</p>
<p id="index_rdd"><code>('aceline', (('K3NLoAEACAAJ', 'keyWords'), ('7DiNBwAAQBAJ', 'title')))</code></p>
<p>'aceline' is the indexed term, K3NLoAEACAAJ is the doc_id, keyWords was the zone and likewise.</p>

<br/>
<h3>Querying</h3>
<p>Each entity needs to have different weights. If 2 documents have the search term present in title and keyWords
    respectively, we have to give more priority to the document whose title matched.
    Likewise, more visited document has to be given higher priority.</p>
<p>Input for this method is the <a href="#index_rdd">output</a> from Index method
    and <a href="#views_rdd">views_rdd</a>.</p>
<p>Following is a weight map which was used in our model.</p>
<p><code>
    MODEL_WEIGHTS = {ISBN_10: .3, ISBN_13: .3, AUTHORS: .18, PUBLISHER: .15, TITLE: .15, CATEGORIES: .12,
    KEYWORDS: .1, VIEWS: .5}
</code></p>
<p>Above weights are formulated by basic logic.</p>
<p>Output of this phase is a list of tuple of document_id and score in sorted order.</p>
<h4>Following outputs show the ranking with view weight exclude and include.</h4>

<p>Without Ranking based on View Count</p>
<code>[('unDKrQEACAAJ', 0.45999999999999996), ('COxdlgEACAAJ', 0.32999999999999996), ('EUrXoAEACAAJ', 0.28), ('r-oGPLclJc0C', 0.28), ('aefUBQAAQBAJ', 0.18), ('g8d3PwAACAAJ', 0.18), ('z73loAEACAAJ', 0.18), ('i-bUBQAAQBAJ', 0.18), ('jUF9BAAAQBAJ', 0.18), ('7cNgPwAACAAJ', 0.18), ('U2P3NwAACAAJ', 0.18), ('dEplHQAACAAJ', 0.18), ('YsE2pwAACAAJ', 0.18), ('NLngYyWFl_YC', 0.18), ('F3anBQAAQBAJ', 0.18), ('eW84rgEACAAJ', 0.18), ('h2xRPgAACAAJ', 0.18), ('46JvnQEACAAJ', 0.18), ('m2QwnwEACAAJ', 0.18), ('JtlDAAAACAAJ', 0.18), ('Jwr8jwEACAAJ', 0.18), ('NdSXoAEACAAJ', 0.18), ('AGAiAgAAQBAJ', 0.15), ('yfnloQEACAAJ', 0.1), ('3QU2SgAACAAJ', 0.1), ('pUGYSQAACAAJ', 0.1), ('Ew--ngEACAAJ', 0.1), ('5kmsSgAACAAJ', 0.1), ('0hX5ygAACAAJ', 0.1), ('ZLN9CQAAQBAJ', 0.1), ('HbxILgEACAAJ', 0.1), ('nunPngEACAAJ', 0.1), ('Zl-lcQAACAAJ', 0.1), ('he2koAEACAAJ', 0.1), ('mmFGSwAACAAJ', 0.1)]</code>

<p>With Ranking based on View Count</p>
<code>[('i-bUBQAAQBAJ', 0.655), ('unDKrQEACAAJ', 0.585), ('h2xRPgAACAAJ', 0.5800000000000001), ('r-oGPLclJc0C', 0.555), ('g8d3PwAACAAJ', 0.5549999999999999), ('pUGYSQAACAAJ', 0.55), ('z73loAEACAAJ', 0.53), ('5kmsSgAACAAJ', 0.525), ('aefUBQAAQBAJ', 0.505), ('jUF9BAAAQBAJ', 0.48), ('46JvnQEACAAJ', 0.43), ('COxdlgEACAAJ', 0.32999999999999996), ('Ew--ngEACAAJ', 0.325), ('ZLN9CQAAQBAJ', 0.30000000000000004), ('EUrXoAEACAAJ', 0.28), ('U2P3NwAACAAJ', 0.28), ('HbxILgEACAAJ', 0.275), ('YsE2pwAACAAJ', 0.255), ('yfnloQEACAAJ', 0.25), ('JtlDAAAACAAJ', 0.22999999999999998), ('NdSXoAEACAAJ', 0.18), ('Jwr8jwEACAAJ', 0.18), ('F3anBQAAQBAJ', 0.18), ('NLngYyWFl_YC', 0.18), ('dEplHQAACAAJ', 0.18), ('7cNgPwAACAAJ', 0.18), ('m2QwnwEACAAJ', 0.18), ('eW84rgEACAAJ', 0.18), ('AGAiAgAAQBAJ', 0.15), ('he2koAEACAAJ', 0.125), ('mmFGSwAACAAJ', 0.1), ('Zl-lcQAACAAJ', 0.1), ('3QU2SgAACAAJ', 0.1), ('0hX5ygAACAAJ', 0.1), ('nunPngEACAAJ', 0.1)]</code>

</body>
</html>